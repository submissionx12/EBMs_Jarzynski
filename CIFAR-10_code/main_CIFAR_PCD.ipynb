{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from Sampler import Sampler\n",
    "from CNNModel import CNNModel\n",
    "from LightningMNISTClassifier import LightningMNISTClassifier\n",
    "import torch\n",
    "from LightningCIFARClassifier import LitResnet\n",
    "from models import ResNetModel\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from utils import GaussianBlur\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../PCD_CIFAR_paper_network_pre\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "fid = FrechetInceptionDistance(normalize = True).to(device)\n",
    "IS = InceptionScore(normalize = True).to(device)\n",
    "\n",
    "import torchvision.transforms.functional as trans_F\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = trans_F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transforms)\n",
    "\n",
    "# indices_car = np.where(np.array(trainset.targets) == 1)[0]\n",
    "# indices_car = indices_car[:2000]\n",
    "# indices_dog = np.where(np.array(trainset.targets) == 5)[0]\n",
    "\n",
    "# indices = np.concatenate((indices_car,indices_dog)).squeeze()\n",
    "\n",
    "# trainset.data = trainset.data[indices]\n",
    "# targets = np.array(trainset.targets)[indices]\n",
    "# targets[targets == 1] = 0\n",
    "# targets[targets == 5] = 1\n",
    "\n",
    "# trainset.targets = targets.tolist()\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True, num_workers=NUM_WORKERS,pin_memory=True)\n",
    "\n",
    "# # Loading the test set\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                       download=True, transform=test_transforms)\n",
    "\n",
    "# indices = np.where((np.array(testset.targets) == 1) | (np.array(testset.targets) == 5))[0]  \n",
    "\n",
    "# testset.data = testset.data[indices]\n",
    "# targets = np.array(testset.targets)[indices]\n",
    "# targets[targets == 1] = 0\n",
    "# targets[targets == 5] = 1\n",
    "# testset.targets = targets.tolist()\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=512,\n",
    "                                         shuffle=False, num_workers=NUM_WORKERS,pin_memory=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Paper_net(torch.nn.Module):\n",
    "    def __init__(self,n_c = 3, n_f = 64, l = 0.2):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(n_c,n_f,3,1,1),\n",
    "            nn.LeakyReLU(l),\n",
    "            nn.Conv2d(n_f,n_f*2,4,2,1),\n",
    "            nn.LeakyReLU(l),\n",
    "            nn.Conv2d(n_f*2,n_f*4,4,2,1),\n",
    "            nn.LeakyReLU(l),\n",
    "            nn.Conv2d(n_f*4,n_f*8,4,2,1),\n",
    "            nn.LeakyReLU(l),\n",
    "            nn.Conv2d(n_f*8,1,4,1,0))\n",
    "    \n",
    "    def forward(self,x): \n",
    "        return self.network(x).squeeze()\n",
    "    \n",
    "    \n",
    "\n",
    "class input_args():\n",
    "    def __init__(self,im_size,filter_dim,norm,spec_norm):\n",
    "        self.im_size = im_size\n",
    "        self.filter_dim = filter_dim\n",
    "        self.norm = norm\n",
    "        self.spec_norm = spec_norm\n",
    "        self.cond = False\n",
    "        self.multiscale = False\n",
    "        self.self_attn = True\n",
    "        self.sigmoid = False\n",
    "        self.square_energy = False\n",
    "\n",
    "class DeepEnergyModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, img_shape, batch_size, alpha= 1e-2, lr=1e-4, steps= 60, step_size= 1, num_batch = 1,\n",
    "                 noise_level = 1e-2, sample_size = 1024, sampler_batch = 256, resample_std = 1, **CNN_args):\n",
    "\n",
    "        \"\"\"\n",
    "        Pytorch Lightning class for the training and validation\n",
    "\n",
    "        Inputs:\n",
    "            img_shape: Shape of the images as tensors (default 1*28*28 for MNIST images)\n",
    "            batch_size - Batch size for drawing data from the training set\n",
    "            alpha - constant which controls the regularization term \n",
    "            lr - Initial learning rate for ADAM\n",
    "            steps - Number of ULA steps for every model parameter update\n",
    "            step_size - ULA step size \n",
    "            noise_level - Noise for ULA\n",
    "            sample_size - The total number of walkers\n",
    "            num_batch - The total number of mini-batches drawn \n",
    "                        and run ULA on in each iteration of parameter update\n",
    "            sampler_batch - number of walkers in each mini-batch\n",
    "            resample_std - critical standard deviation for adaptive resampling\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "#         pretrained_filename = \"CIFAR10_checkpoint.ckpt\" # load the pre-trained classifer\n",
    "#         self.classifier = LitResnet.load_from_checkpoint(pretrained_filename)\n",
    "#         self.classifier.eval()\n",
    "        self.args = input_args(32,64,True,False)\n",
    "        \n",
    "        self.cnn = Paper_net()\n",
    "        self.sampler = Sampler(self.cnn, img_shape=img_shape,sample_size = self.hparams.sample_size)\n",
    "\n",
    "        self.langevin_steps = 0\n",
    "        \n",
    "\n",
    "        # obtain the images on the full manufactured training set for \n",
    "        # estimation of the cross entropy \n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        data_loader = torch.utils.data.DataLoader(trainset, batch_size=trainset.data.shape[0],\n",
    "                                         shuffle=True, num_workers=4,pin_memory=True)\n",
    "        \n",
    "        for batch_idx, samples in enumerate(data_loader):\n",
    "              self.data = samples[0].to(device)\n",
    "        \n",
    "#         indices_car_reduced = indices_car[:100]\n",
    "#         indices_dog_reduced = indices_dog[:40]\n",
    "\n",
    "#         indices_reduced = np.concatenate((indices_car_reduced,indices_dog_reduced)).squeeze()\n",
    "        \n",
    "        self.reduced_train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                download=True, transform=train_transforms)\n",
    "        \n",
    "        random_indices = torch.multinomial(torch.ones((self.reduced_train_set.data.shape[0],)),300,replacement = False).to(device)\n",
    "        \n",
    "        \n",
    "        data_loader = data.DataLoader(trainset, batch_size=trainset.data.shape[0], shuffle=True,  drop_last=False,  num_workers=4, pin_memory=True)\n",
    "        reduced_data_loader = data.DataLoader(self.reduced_train_set, batch_size=self.reduced_train_set.data.shape[0], shuffle=True,  drop_last=False,  num_workers=4, pin_memory=True)\n",
    "        \n",
    "        for batch_idx, samples in enumerate(data_loader):\n",
    "              self.data = samples[0].to(device)\n",
    "                \n",
    "        for batch_idx, samples in enumerate(reduced_data_loader):\n",
    "              self.reduced_data = samples[0].to(device)\n",
    "                \n",
    "        inverse_transform =  transforms.Compose([transforms.Normalize((0,0,0), (2,2,2)),\n",
    "                                transforms.Normalize((-0.5, -0.5, -0.5), (1,1,1))\n",
    "                               ])\n",
    "        \n",
    "        FID_set = self.reduced_data[random_indices]\n",
    "        self.data_uint = FID_set\n",
    "        \n",
    "        \n",
    "        fid.update(self.data_uint, real=True)\n",
    "        \n",
    "        print(\"Data is loaded\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.sampler.ce = ((-self.cnn(self.sampler.examples)).exp().mean()).log() + self.cnn(self.data).mean()\n",
    "            self.sampler.normalization = (-self.cnn(self.sampler.examples)).exp().mean()\n",
    "            self.sampler.normal_0 = self.sampler.normalization.clone().detach()\n",
    "            \n",
    "        # run Langevin dynamics on the walkers without updating their weights\n",
    "        # to make sure they are samples from the initial distribution ~ exp(-U_\\theta)\n",
    "        self.sampler.examples = self.sampler.pure_generate_samples(self.cnn,self.sampler.examples,\n",
    "                                                                   steps=2000, \n",
    "                                                                   step_size=self.hparams.step_size,\n",
    "                                                                   noise_level = self.hparams.noise_level)\n",
    "        \n",
    "\n",
    "        self.langevin_steps = 0\n",
    "        \n",
    "        index = torch.multinomial(torch.ones((self.hparams.sample_size,)),self.hparams.sampler_batch,replacement = False).to(device)\n",
    "        self.fake_imgs = self.sampler.examples[index]\n",
    "#         self.fake_imgs_weights = self.sampler.log_weights[index]\n",
    "        print(\"Training begins\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)\n",
    "        return z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        # Energy models can have issues with momentum as the loss surfaces changes with its parameters.\n",
    "        # Hence, we set it to 0 by default.\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr, betas=(0.0, 0.999))\n",
    "        \n",
    "        initial_lr = 1\n",
    "        min_lr = 0\n",
    "        n_iter = 600\n",
    "        \n",
    "        lambda1 = lambda epoch: max((initial_lr - epoch*(initial_lr - min_lr)/n_iter),1e-10) # linear decay\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "        \n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        real_imgs, _ = batch\n",
    "        \n",
    "        noise = torch.randn(real_imgs.shape, device=real_imgs.device)\n",
    "        noise.normal_(0, 0.03)\n",
    "        \n",
    "        real_imgs = real_imgs + noise.data\n",
    "        \n",
    "        # Obtain samples from the set of walkers\n",
    "        \n",
    "        \n",
    "        self.langevin_steps = self.langevin_steps + self.hparams.steps\n",
    "        \n",
    "        # Predict the energy for all images\n",
    "        real_out = self.cnn(torch.cat([real_imgs], dim=0))\n",
    "        fake_out = self.cnn(torch.cat([self.fake_imgs], dim=0))\n",
    "        \n",
    "        # Calculate losses\n",
    "\n",
    "#         weights = self.fake_imgs_weights.clone().exp().detach()\n",
    "        \n",
    "        reg_loss = self.hparams.alpha * ((real_out ** 2).mean() + (fake_out ** 2).mean())\n",
    "        cdiv_loss = -fake_out.mean() + real_out.mean()\n",
    "    \n",
    "        \n",
    "        loss = cdiv_loss + reg_loss\n",
    "        \n",
    "#         print(loss)\n",
    "        \n",
    "        # track the gradients of the parameters\n",
    "\n",
    "        parameters = [p for p in self.cnn.parameters() if p.grad is not None and p.requires_grad]\n",
    "        if len(parameters) == 0:\n",
    "            total_norm = 0.0\n",
    "        else:\n",
    "            total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach()) for p in parameters]))\n",
    "        \n",
    "        \n",
    "        langevin_steps = self.sampler.langevin_steps\n",
    "        \n",
    "        # Do ULA and weight updates on the walkers\n",
    "        self.fake_imgs,self.fake_imgs_weights = self.sampler.sample_new_exmps(steps=self.hparams.steps, \n",
    "                                                  step_size=self.hparams.step_size,\n",
    "                                                  noise_level = self.hparams.noise_level,\n",
    "                                                  batch_size = self.hparams.sampler_batch,\n",
    "                                                  num_batch = self.hparams.num_batch)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             self.sampler.ce = self.sampler.normalization.log() + self.cnn(self.data).mean()\n",
    "#             probability_weights = weights/weights.sum()\n",
    "#             self.sampler.ess = 1/((probability_weights**2).sum())\n",
    "            \n",
    "#         if torch.std(self.sampler.weights/self.sampler.weights.mean()) > self.hparams.resample_std and self.current_epoch > 300:\n",
    "#             self.sampler.resample_multinomial()\n",
    "            \n",
    "        \n",
    "        # Logging\n",
    "        self.log('loss', loss)\n",
    "        self.log('loss_regularization', reg_loss)\n",
    "        self.log('loss_contrastive_divergence', cdiv_loss)\n",
    "\n",
    "        self.log('energy_avg_real', real_out.mean())\n",
    "        self.log('energy_avg_fake', fake_out.mean())\n",
    "\n",
    "#         self.log('largest_log_weight',self.sampler.log_weights.max() )\n",
    "#         self.log('smallest_log_weight',self.sampler.log_weights.min() )\n",
    "#         self.log('mean_log_weight',self.sampler.log_weights.mean() )\n",
    "\n",
    "        self.log('f_norm',total_norm)\n",
    "        self.log('langevin_steps',langevin_steps)\n",
    "#         self.log('weight_std',torch.std(self.sampler.weights/self.sampler.weights.mean()))\n",
    "#         self.log('ess',self.sampler.ess)\n",
    "        \n",
    "#         self.log('cross_entropy',self.sampler.ce)\n",
    "#         self.log('normalization',self.sampler.normalization)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        inverse_transform =  transforms.Compose([transforms.Normalize((0,0,0), (2,2,2)),\n",
    "                                transforms.Normalize((-0.5, -0.5, -0.5), (1,1,1))\n",
    "                               ])\n",
    "        \n",
    "        example_indices = torch.randint(0,self.hparams.sample_size-1,(300,))\n",
    "        example = self.sampler.examples[example_indices]\n",
    "\n",
    "        random_indices = torch.multinomial(torch.ones((self.reduced_train_set.data.shape[0],)),300,replacement = False).to(device)\n",
    "        \n",
    "        FID_set = self.reduced_data[random_indices]\n",
    "        fid.update(FID_set, real=True)\n",
    "        \n",
    "        fid.update(example, real=False)\n",
    "        FID_score = fid.compute()\n",
    "        IS.update(self.sampler.examples)\n",
    "        IS_score = IS.compute()\n",
    "        \n",
    "        self.log('FID',FID_score)\n",
    "        self.log(\"Inception Score_mean\",IS_score[0])\n",
    "        self.log(\"Inception Score_std\",IS_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SamplerCallback(pl.Callback):\n",
    "    \n",
    "    def __init__(self, num_imgs=128, every_n_epochs=4):\n",
    "        super().__init__()\n",
    "        self.num_imgs = num_imgs             # Number of images to plot\n",
    "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "        \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % 10 == 0:\n",
    "            \n",
    "            print(trainer.current_epoch)\n",
    "            torch.set_grad_enabled(True)  # Tracking gradients for sampling necessary\n",
    "            exmp_imgs = pl_module.sampler.examples\n",
    "            indices = torch.randint(0,exmp_imgs.shape[0],size = (self.num_imgs,))\n",
    "            exmp_imgs = exmp_imgs[indices].clone().detach()\n",
    "            \n",
    "            all_imgs = pl_module.sampler.examples.clone().detach()\n",
    "            all_weights = pl_module.sampler.weights.clone().detach()\n",
    "            save_data = {'images': all_imgs, \"weights\":all_weights}\n",
    "            foldername = \"PCD_save_new_all_/\" # folder for saving all the data\n",
    "            if not os.path.exists(foldername): # if not exists, create one\n",
    "                os.makedirs(foldername)\n",
    "            filename = foldername + \"images_resample\" + str(trainer.current_epoch) + \"_.pt\"\n",
    "            torch.save(save_data,filename)\n",
    "            grid = torchvision.utils.make_grid(exmp_imgs, nrow=8, normalize=True, value_range=(-1,1))\n",
    "            trainer.logger.experiment.add_image(\"sampler\", grid, global_step=trainer.current_epoch)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(**kwargs):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"MNIST\"),\n",
    "                         accelerator='gpu', devices=1,\n",
    "                         max_epochs=600,\n",
    "                         log_every_n_steps=10,\n",
    "                         gradient_clip_val=1000,\n",
    "                         # profiler=\"simple\",\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor='FID'),\n",
    "                                    SamplerCallback(every_n_epochs=2),\n",
    "                                    LearningRateMonitor(\"epoch\")\n",
    "                                   ])\n",
    "\n",
    "    pl.seed_everything(42)\n",
    "    model = DeepEnergyModel(**kwargs)\n",
    "    trainer.fit(model, trainloader, testloader)\n",
    "    model = DeepEnergyModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    \n",
    "    # No testing as we are more interested in other properties\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(img_shape=(3,32,32), \n",
    "                    batch_size=trainloader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_2",
   "language": "python",
   "name": "my_env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
