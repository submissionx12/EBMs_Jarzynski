{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code portion was in the main notebook, right before the def of the folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=0\n",
    "\n",
    "if dim==2:\n",
    "    print(\"Dimension 2\")\n",
    "    loss_all = torch.zeros(int(n_iter)) \n",
    "    regularization_all_data = torch.zeros(int(n_iter))\n",
    "    regularization_all_xw = torch.zeros(int(n_iter))\n",
    "\n",
    "    G3_s = model(test_all).detach().flatten()\n",
    "    G3_t = teacher(test_all).detach()\n",
    "\n",
    "    Z3_t = torch.sum(torch.exp(-G3_t)*((x_limit*2/(n_sample-1))**2)).detach() # normalization constant for the teacher model\n",
    "    lZ3_t = torch.log(Z3_t).detach() # log of the normalization constant\n",
    "\n",
    "    Z3_s = torch.sum(torch.exp(-G3_s)*((x_limit*2/(n_sample-1))**2)).detach()\n",
    "    lZ3_s = torch.log(Z3_s).detach()\n",
    "    KL[0] = torch.sum((G3_s - G3_t )*torch.exp(-G3_t))*((x_limit*2/(n_sample-1))**2) + lZ3_s\n",
    "\n",
    "    G3_s = model2(test_all).detach().flatten()\n",
    "    G3_t = teacher(test_all).detach()\n",
    "\n",
    "    Z3_t = torch.sum(torch.exp(-G3_t)*((x_limit*2/(n_sample-1))**2)).detach() # normalization constant for the teacher model\n",
    "    lZ3_t = torch.log(Z3_t).detach() # log of the normalization constant\n",
    "\n",
    "    Z3_s = torch.sum(torch.exp(-G3_s)*((x_limit*2/(n_sample-1))**2)).detach()\n",
    "    lZ3_s = torch.log(Z3_s).detach()\n",
    "    KL2[0] = torch.sum((G3_s - G3_t )*torch.exp(-G3_t))*((x_limit*2/(n_sample-1))**2) + lZ3_s \n",
    "\n",
    "    add_const= torch.sum(( - G3_t )*torch.exp(-G3_t))*((x_limit*2/(n_sample-1))**2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code was in the middle of the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t%1000==0:\n",
    "        \n",
    "        if dim==2:\n",
    "          G3_s = model(test_all).detach().flatten()\n",
    "\n",
    "          Z3_s = torch.sum(torch.exp(-G3_s)*((x_limit*2/(n_sample-1))**2)).detach()\n",
    "          lZ3_s = torch.log(Z3_s).detach() \n",
    "          KL[q] = torch.sum((G3_s - G3_t )*torch.exp(-G3_t))*((x_limit*2/(n_sample-1))**2)+ lZ3_s\n",
    "\n",
    "          G3_s = model2(test_all).detach().flatten()\n",
    "          Z3_s = torch.sum(torch.exp(-G3_s)*((x_limit*2/(n_sample-1))**2)).detach()\n",
    "          lZ3_s = torch.log(Z3_s).detach() \n",
    "\n",
    "          KL2[q] = torch.sum((G3_s - G3_t )*torch.exp(-G3_t))*((x_limit*2/(n_sample-1))**2)+ lZ3_s\n",
    "\n",
    "        ce[q]= part_func[q]+model(data).detach().mean()  \n",
    "        q=q+1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting util inside the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dim==2:\n",
    "            plot_observables_2d(model, \n",
    "                        model2, \n",
    "                        teacher, \n",
    "                        test_all, \n",
    "                        x1_sample, \n",
    "                        x2_sample, \n",
    "                        n_sample, \n",
    "                        xw, \n",
    "                        xw_compare, \n",
    "                        data, \n",
    "                        foldername, \n",
    "                        KL, \n",
    "                        KL2, \n",
    "                        ce, \n",
    "                        part_func)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next snippet is not specific to 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if teach:\n",
    "  np.save(foldername+'model_mean',model.mean.cpu().numpy())\n",
    "  np.save(foldername+'teacher_mean',teacher.mean.cpu().numpy())\n",
    "  np.save(foldername+'model_logstd',model.log_std.cpu().numpy())\n",
    "  np.save(foldername+'teacher_logstd',teacher.log_std.cpu().numpy())\n",
    "  np.save(foldername+'model_mass',model.mix_logits.cpu().numpy())\n",
    "  np.save(foldername+'teacher_mass',teacher.mix_logits.cpu().numpy())\n",
    "  np.save(foldername+'walkers_jarz',xw.cpu().numpy())\n",
    "  np.save(foldername+'walkers_nojarz',xw_compare.cpu().numpy())\n",
    "  np.save(foldername+'weights',log_weight.cpu().numpy())   \n",
    "  np.save(foldername+'KLJ',KL.cpu().numpy()) \n",
    "  np.save(foldername+'KLnoJ',KL2.cpu().numpy()) \n",
    "  np.save(foldername+'ceJ',ce.cpu().numpy()) \n",
    "  np.save(foldername+'partition',part_func.cpu().numpy()) \n",
    "  torch.save(model, foldername+'modelJ')\n",
    "  torch.save(teacher, foldername+'teacher')\n",
    "  torch.save(model2, foldername+'modelnoJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if dim==2:\n",
    "    fig5 = plt.figure(4);fig5.clf\n",
    "    plt.semilogy(KL.detach().cpu().numpy()[0:q],label = \"W/ Jarzynski - On Grid\")\n",
    "    plt.semilogy(KL2.detach().cpu().numpy()[0:q],label = \"W/o Jarzynski - On Grid\")\n",
    "    plt.semilogy(ce.detach().cpu().numpy()[0:q]+add_const.cpu().numpy(),label = \"W/ Jarzynski - Using weights\")\n",
    "    #         plt.semilogy(total_norm_all.cpu().numpy()[0:t],label = \"Regularization W/ Jarzynski\")\n",
    "    plt.legend()\n",
    "    plt.title(\"KL Divergence\")\n",
    "    plt.xlabel(\"Adam Iterations x 1e3\")\n",
    "    plt.ylabel(\"KL\")\n",
    "    filename = str(t) + \"_KL.png\"\n",
    "    plt.savefig(foldername + filename, dpi=300, bbox_inches='tight', transparent=True,facecolor='w')\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
