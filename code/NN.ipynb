{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "from models import GMM_teacher, GMM\n",
    "from loss import WLoss\n",
    "from plotting_utils import plot_observables\n",
    "from models import Walkers, WeightedWalkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This can be written in a smarter way like in the algorithm on the paper        \n",
    "\n",
    "seed=0\n",
    "teach=False\n",
    "sched=False\n",
    "resamp=True\n",
    "max_var=1\n",
    "contr=False\n",
    "zeros=True\n",
    "L2=0\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.manual_seed(seed)\n",
    "dim = 2 # dimension\n",
    "n_iter = int(4e3) # total number of GD iterations\n",
    "hx = 1e-3 # time stepsize for walkers x\n",
    "n_walker = 5000  # number of walkers\n",
    "lr =5e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = GMM_teacher(dim,device=device) # teacher model\n",
    "model = GMM(dim,hidden_dim = 7, device=device).to(device) # MLP model \n",
    "model2 = GMM(dim,hidden_dim = 7, device=device).to(device) # MLP model\n",
    "wloss_func = WLoss()\n",
    "\n",
    "data = teacher.sample(1e3).to(device) # sample data\n",
    "data_KL = teacher.sample(3e3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizers\n",
    "optimizer = optim.Adam(model.parameters(), lr= lr,weight_decay=L2) # ADAM \n",
    "optimizer_compare = optim.Adam(model2.parameters(), lr = lr,weight_decay=L2) # ADAM \n",
    "\n",
    "initial_lr = 1\n",
    "min_lr = 2e-7\n",
    "fact=0.99\n",
    "\n",
    "#schedulers\n",
    "\n",
    "# lambda1 = lambda epoch: max(0.99998 ** epoch, min_lr) # exponential decay \n",
    "lambda1 = lambda epoch: (initial_lr - epoch*(initial_lr - min_lr)/n_iter) # linear decay\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',verbose=True,patience=1500,factor=fact)#torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "scheduler_compare = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',verbose=True,patience=1500,factor=fact)#torch.optim.lr_scheduler.LambdaLR(optimizer_compare, lr_lambda=lambda1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_norm_all = torch.zeros([n_iter, 1],device=device) # monitor the L1 norm of gradients of parameters\n",
    "\n",
    "W = WeightedWalkers(init_data=model.sample(n_walker), hx=hx, device=device, clip_lim=100, use_resampling=True, max_var=max_var)\n",
    "W_PCD = Walkers(init_data=model2.sample(n_walker), hx=hx, device=device, clip_lim = 100) # not sure about the 100\n",
    "\n",
    "# approximation of integral of \\rho log \\rho as sum on modes of p_i\\log(p_i)+p_i Integral(\\rho_i \\log (\\rho_i))\n",
    "# https://gregorygundersen.com/blog/2020/09/01/gaussian-entropy/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_weight = torch.zeros((n_walker,),device=device)\n",
    "logwal=torch.tensor([np.log(n_walker)],device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 800 # construct a uniform grid in R^2\n",
    "\n",
    "x_limit = 100\n",
    "y_limit = 100\n",
    "\n",
    "KL = torch.zeros(int(n_iter/1000)) \n",
    "KL2 = torch.zeros(int(n_iter/1000)) \n",
    "\n",
    "ce=torch.zeros(int(n_iter/1000)) \n",
    "ce2=torch.zeros(int(n_iter/1000))\n",
    "old_energy = model(W.walkers).clone().detach()\n",
    "part_func=torch.zeros(int(n_iter), requires_grad=False)\n",
    "\n",
    "x1_sample = np.linspace(-x_limit,x_limit,num=n_sample)\n",
    "x2_sample = np.linspace(-x_limit,x_limit,num=n_sample)\n",
    "\n",
    "x1_meshgrid,x2_meshgrid = np.meshgrid(x1_sample,x2_sample)\n",
    "\n",
    "x1 = np.reshape(x1_meshgrid,(1,np.power(n_sample,2)))\n",
    "x2 = np.reshape(x2_meshgrid,(1,np.power(n_sample,2)))\n",
    "\n",
    "test_all = torch.t(torch.from_numpy(np.concatenate((x1,x2,np.zeros((dim-2,x1.shape[1]))))).float()).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=0\n",
    "\n",
    "foldername = \"NN_fromzero_savez_\"+str(zeros)+\"_sched_\"+str(sched)+\"resampl\"+str(resamp)+\"var\"+str(max_var)+\"_teachstudGMM\"+str(dim)+\"d_seed\"+str(seed)+\"_lr\"+str(lr)+\"_ULA\"+str(hx)+\"_plat\"+str(fact)+\"_reg_+\"+str(L2)+\"/\" # folder for saving all \n",
    "#the data\n",
    "if not os.path.exists(foldername): # if not exists, create one\n",
    "    os.makedirs(foldername)\n",
    "\n",
    "log_Zres=torch.zeros(1,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83426/2193749696.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for t in tqdm(range(n_iter)): # main loop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86605d62da7d4df48b97be6f649d4285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(203.4473, device='cuda:0', grad_fn=<SubBackward0>) tensor(204.0476, device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(202.9598, device='cuda:0', grad_fn=<SubBackward0>) tensor(203.5896, device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(202.9134, device='cuda:0', grad_fn=<SubBackward0>) tensor(203.5756, device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(202.9581, device='cuda:0', grad_fn=<SubBackward0>) tensor(203.6526, device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(202.2712, device='cuda:0', grad_fn=<SubBackward0>) tensor(202.9957, device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(202.7197, device='cuda:0', grad_fn=<SubBackward0>) tensor(203.4774, device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m#    PCD : Langevin ULA \u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m#W_PCD.requires_grad(True)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model2\u001b[39m.\u001b[39mrequires_grad(\u001b[39mFalse\u001b[39;00m) \n\u001b[0;32m---> 29\u001b[0m W_PCD\u001b[39m.\u001b[39;49mLangevin_step(model2)\n\u001b[1;32m     30\u001b[0m model2\u001b[39m.\u001b[39mrequires_grad(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m loss_compare \u001b[39m=\u001b[39m wloss_func(model2(data), model2(W_PCD\u001b[39m.\u001b[39mold_walkers))\n",
      "File \u001b[0;32m~/Documents/EBMs_Jarzynski/code/models.py:141\u001b[0m, in \u001b[0;36mWalkers.Langevin_step\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    139\u001b[0m model\u001b[39m.\u001b[39mrequires_grad(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 141\u001b[0m energy \u001b[39m=\u001b[39m model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwalkers)\n\u001b[1;32m    142\u001b[0m energy\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemorize() \u001b[39m# right before the Langevin\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/EBMs_Jarzynski/code/models.py:88\u001b[0m, in \u001b[0;36mGMM.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m     87\u001b[0m     \u001b[39m#initially a gaussian in zero to ensure that walkers are sampled from \\rho_0\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     energy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mneuralnet(X) \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA\u001b[39m.\u001b[39mexp())\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA\u001b[39m.\u001b[39mexp())\u001b[39m*\u001b[39m(X\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvar)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,keepdim \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m energy\n",
      "File \u001b[0;32m~/miniconda3/envs/MLenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/MLenv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1494\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m             tracing_state\u001b[39m.\u001b[39mpop_scope()\n\u001b[1;32m   1492\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m-> 1494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1495\u001b[0m     forward_call \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state() \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward)\n\u001b[1;32m   1496\u001b[0m     \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m     \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in tqdm(range(n_iter)): # main loop\n",
    "\n",
    "    # Preparing for Langevin on walkers and data\n",
    "    \n",
    "    data.requires_grad = True\n",
    "    data=teacher.sample(1e3).to(device)\n",
    "    data.requires_grad = False \n",
    "\n",
    "    #   Weighted-CD :Â using Jarczinski\n",
    "    model.requires_grad(False)\n",
    "    W.Langevin_step(model)\n",
    "    log_weight_update_1 = W.compute_delta(model)#alpha k (x_k,x_{k+1}) \n",
    "    normalized_weights = W.get_normalized_weigths()\n",
    "    model.requires_grad(True)\n",
    "    loss = wloss_func(model(data), model(W.old_walkers), normalized_weights)\n",
    "    loss.backward() # optimize the parameters \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad() # clean the grad of parameters \n",
    "    part_func[t]=torch.logsumexp(W.log_weights,0)-logwal+log_Zres   \n",
    "    model.requires_grad(False)\n",
    "    log_weight_update_2 = W.compute_delta(model)     #alpha_{k+1}(x_{k+1},x_k)\n",
    "    W.update_weights(log_weight_update_2 - log_weight_update_1)\n",
    "    W.resample() #resampling step\n",
    "\n",
    "    #    PCD : Langevin ULA \n",
    "    model2.requires_grad(False) \n",
    "    W_PCD.Langevin_step(model2)\n",
    "    model2.requires_grad(True)\n",
    "    loss_compare = wloss_func(model2(data), model2(W_PCD.old_walkers))\n",
    "    loss_compare.backward()\n",
    "    optimizer_compare.step()\n",
    "    optimizer_compare.zero_grad() # clean the grad of parameters \n",
    "\n",
    "    if t%200==0: \n",
    "        print(loss_compare, loss)\n",
    "\n",
    "    #Scheduler\n",
    "#     if sched==True and t>n_iter/2:\n",
    "#         scheduler.step(loss) # adjust the learning rate \n",
    "#         scheduler_compare.step(loss_compare) # adjust the learning rate \n",
    "         \n",
    "    # plotting \n",
    "    if t % 20000== 0 and t>0:\n",
    "        \n",
    "        plot_observables(\n",
    "                model, \n",
    "                model2, \n",
    "                teacher, \n",
    "                test_all, \n",
    "                x1_sample, \n",
    "                x2_sample, \n",
    "                n_sample, \n",
    "                W.walkers, \n",
    "                W_PCD.walkers, \n",
    "                foldername)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_foldername=foldername+'data/' \n",
    "if not os.path.exists(data_foldername): # if not exists, create one\n",
    "    os.makedirs(data_foldername)\n",
    "\n",
    "else:\n",
    "  np.save(data_foldername+'teacher_mean',teacher.mean.cpu().numpy())\n",
    "  np.save(data_foldername+'teacher_logstd',teacher.log_std.cpu().numpy())\n",
    "  np.save(data_foldername+'teacher_mass',teacher.mix_logits.cpu().numpy())\n",
    "  np.save(data_foldername+'walkers_jarz',W.walkers.cpu().numpy())\n",
    "  np.save(data_foldername+'walkers_nojarz',W_PCD.walkers.cpu().numpy())\n",
    "  np.save(data_foldername+'weights',log_weight.cpu().numpy())\n",
    "  np.save(data_foldername+'KLJ',KL.cpu().numpy()) \n",
    "  np.save(data_foldername+'KLnoJ',KL2.cpu().numpy()) \n",
    "  np.save(data_foldername+'ceJ',ce.cpu().numpy()) \n",
    "\n",
    "  np.save(data_foldername+'partition',part_func.detach().cpu().numpy()) \n",
    "  torch.save(model, data_foldername+'modelJ')\n",
    "  torch.save(teacher, data_foldername+'teacher')\n",
    "  torch.save(model2, data_foldername+'modelnoJ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "fig, axes = plt.subplots(1,3, figsize=(12,4))\n",
    "area = 10\n",
    "Energy_landscape = (model(test_all)-model(test_all).min()).detach() \n",
    "levels = np.linspace(0,30,20)\n",
    "axes[0].set_title(\"Energy Function (w/ Jarzynski)\")\n",
    "contour = axes[0].contourf(x1_sample,x2_sample,Energy_landscape.cpu().numpy().reshape((n_sample,n_sample)),levels = levels)\n",
    "cbar3 = plt.colorbar(contour)\n",
    "axes[0].set_xlim(-100,100)\n",
    "axes[0].set_ylim(-100,100)\n",
    "\n",
    "Energy_landscape = (model2(test_all)-model2(test_all).min()).detach() \n",
    "contour = axes[1].contourf(x1_sample,x2_sample,Energy_landscape.cpu().numpy().reshape((n_sample,n_sample)),levels = levels)\n",
    "axes[1].set_title(\"Energy Function (w/o Jarzynski)\")\n",
    "cbar3 = plt.colorbar(contour)\n",
    "axes[1].set_xlim(-100,100)\n",
    "axes[1].set_ylim(-100,100)\n",
    "\n",
    "Energy_landscape = (teacher(test_all)-teacher(test_all).min()).detach() \n",
    "contour = axes[2].contourf(x1_sample,x2_sample,Energy_landscape.cpu().numpy().reshape((n_sample,n_sample)),levels = levels)\n",
    "axes[2].set_title(\"True Energy Function\")\n",
    "axes[2].set_xlim(-100,100)\n",
    "axes[2].set_ylim(-100,100)\n",
    "filename = str(t) + \"_data.png\"\n",
    "plt.savefig(foldername + filename, dpi=300, bbox_inches='tight', transparent=True,facecolor='w')\n",
    "plt.close()\n",
    "\n",
    "# fig4 = plt.figure(4);fig4.clf\n",
    "# min1=np.abs(np.min(ce.detach().cpu().numpy()[0:t]))\n",
    "# min2=np.abs(np.min(ce2.detach().cpu().numpy()[0:t]))\n",
    "# shift=np.max((min1,min2))+0.1\n",
    "# plt.semilogy(ce.detach().cpu().numpy()[0:t]+shift,label = \"W/ Jarzynski\")\n",
    "# plt.semilogy(ce2.detach().cpu().numpy()[0:t]+shift,label = \"W/o Jarzynski\")\n",
    "# plt.title(\"Loss\")\n",
    "# plt.xlabel(\"Adam Iterations\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# filename = str(t) + \"_ce.png\"\n",
    "# plt.savefig(foldername + filename, dpi=300, bbox_inches='tight', transparent=True,facecolor='w')\n",
    "# plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
