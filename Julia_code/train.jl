
using ProgressBars
using EllipsisNotation
using Base:@kwdef
using StaticArrays
using BSON: @load, @save


function gm(N,p;a=-10, b=9)
    """
    Gaussian mixture p N(a, 1) + (1 - p)N(b, 1)
    """
    x,y = randn(N), rand(N).<p
    return x .+ b .+ (a-b).*y
end

g(x) = exp.(-0.5*abs.(x).^2)/âˆš(2*Ï€) # Gaussian density
Ïƒ(s) = s<=0 ? exp(s)/(1 + exp(s)) : 1/(1 + exp(-s)) # efficient sigmoid
dÏƒ(s) = s<=0 ? exp(s) / (1 + exp(s))^2 : 1/(2+exp(s) + exp(-s)) # sigmoid derivative
Ïƒinv(p) = log(p/(1-p)) # sigmoid inverse

@kwdef struct Args 
    N_epochs::Int
    dt::Float32 = .01 # time discretization
    Î·::Float32 = .05 # GDÂ step size (learning rate)
    N::Int = 1000
    K::Int = 1000
    stride::Int = 1000
end
    
abstract type AbstractSystem end

mutable struct OracleSystem <: AbstractSystem
    x #array
    n::Int
    s::Float32 
    p::Float32
    a::Float32
    b::Float32
    âˆ‡
end

function OracleSystem(;n=1000, s=-0.9, a=-10., b=6.) # training data
    p=Ïƒ(s)
    x = gm(n,p; a=a, b=b)
    âˆ‡ = âˆ‡U(x, s, a, b)
    OracleSystem(x,n,s,p,a,b,âˆ‡)
end

mutable struct WeightedSystem <: AbstractSystem # walkers
    x #array
    w # weights (not exponential weights)
    n::Int
    s::Float32 
    p::Float32
    a::Float32
    b::Float32
    âˆ‡
    Î´
end

function WeightedSystem(;n=1000,s=0.1,a=-0.5,b=0.2)
    p = Ïƒ(s)
    x = gm(n,p;Â a=a, b=b)
    w = zeros(n)
    âˆ‡ = âˆ‡U(x,s,a,b)
    Î´ = zeros(1,3)
    WeightedSystem(x,w,n,s,p,a,b, âˆ‡, Î´) 
end


function U(x,s,a,b)
    """Returns - log â„™(X=x) where X is a Gaussian mixture: p N(a,1) + (1-p)N(b,1), 
    where p = Ïƒ(s).  
    Beware of the sign: this is -log â„™. """
    p = Ïƒ(s)
    prob = p*g(x.-a) .+ (1-p)*g(x.-b)
    return -log.(prob)
end

function âˆ‚â‚“U(x,s,a,b) #gradient in x
    """Returns only the gradient is x. """
    p = Ïƒ(s)
    ga, gb = g(x.-a), g(x.-b)
    numerator = p*ga .+  (1-p)*gb
    âˆ‚x = (-p*(x.-a).*ga - (1-p)*(x.-b).*gb ) ./ numerator
    return -âˆ‚x
end

function âˆ‡U(x,s,a,b) 
    """ âˆ‡U where U = -log â„™.  
    Beware of the sign: this is âˆ‡(-log â„™).   That is, we suppose that â„™ âˆ exp(-U). 
    Returns: âˆ‚s, âˆ‚a, âˆ‚b, âˆ‚x"""
    p = Ïƒ(s)
    ga, gb = g(x.-a), g(x.-b)
    numerator = p*ga .+  (1-p)*gb

    âˆ‚s = dÏƒ(s)* (ga - gb) ./ numerator
    âˆ‚a = p*(x.-a).*ga ./ numerator
    âˆ‚b = (1-p)*(x.-b).*gb ./ numerator

    return [-âˆ‚s -âˆ‚a -âˆ‚b]
end

âˆ‡U(S::AbstractSystem) = âˆ‡U(S.x, S.s, S.a, S.b)


function diffuse!(x, gradient, dt)
    """One Langevin diffusion step with step size Ï„:Â 
    x âŸµ x - Ï„*gradient + ð’©(0,2Ï„). """
    x[:] = x .- dt*gradient .+ âˆš(2*dt).*randn(size(x)...)
    nothing
end

diffuse!(S::AbstractSystem, dt) = diffuse!(S.x, âˆ‚â‚“U(S.x, S.s, S.a, S.b), dt)

function create_system(;sâ‚€=-0.9,N,K, Î· = 0.05, Ï„=0.01, weighted=true)
    @info "System is created"
    aâ‚€,bâ‚€ = -10, 6
    a,b = -10, 6
    s = 0.1
    x = gm(K,Ïƒ(s),a=a,b=b)

    #x = cat(randn(Int(K/2)).+a, randn(Int(K/2)).+b, dims=1)
    p = Ïƒ(sâ‚€)
    xâ‚€ = gm(N,p,a=aâ‚€,b=bâ‚€)
    if weighted
        w = zeros(K)
        moments = zeros(K, 3)
        student = âˆ‡U(x,s,a,b)
        teacher = âˆ‡U(xâ‚€, s, a, b)
        return WeightedSystem(x, xâ‚€, s, Î·, Ï„, a,b, aâ‚€, bâ‚€, sâ‚€, w, student, teacher, 0., 0., 0., moments)
    else
        return System(x, xâ‚€, 0.1, Î·, Ï„, a,b, aâ‚€, bâ‚€, sâ‚€)
    end
end

function compute_q(S::WeightedSystem)::Float32
    W = exp.(S.w)
    q = sum( ( S.x .> 0 ) .* W ) / sum(W)
    return q
end

mutable struct History 
    s::Vector{Float32}
    p::Vector{Float32}
    a::Vector{Float32}
    b::Vector{Float32}
    Î´::Matrix{Float32}
    w::Matrix{Float32}
    q::Vector{Float32}
    t
    args::Args
    qhat::Float32
    ahat::Float32
    bhat::Float32
    pzero::Float32
    oracle::OracleSystem
end

function create_logger(args::Args, T::OracleSystem)
    N_logged = args.N_epochsÃ·args.stride
    s = zeros(Float32, N_logged)
    p = zeros(Float32, N_logged)
    a = zeros(Float32, N_logged)
    b = zeros(Float32, N_logged)
    Î´ = zeros(Float32, N_logged, 3)
    w = zeros(Float32, N_logged, args.K)
    q = zeros(Float32, N_logged)
    t = 1:args.stride:args.N_epochs
    pzero = -1.0
    oracle = T
    return History(s,p,a,b,Î´,w,q,t,args,0.0,0.0,0.0,pzero,oracle)
end

function log!(idx::Int, L::History, S::WeightedSystem)
    L.s[idx] = S.s
    L.p[idx] = S.p
    L.a[idx] = S.a
    L.b[idx] = S.b
    L.Î´[idx, :] = S.Î´
    L.w[idx, :] = S.w
    L.q[idx] = compute_q(S)
    nothing
end

function update!(
    S::WeightedSystem, 
    T::OracleSystem, 
    dt, 
    Î·, 
    update_ab, 
    update_jarczynski,
    )
    """Main function for the update of the system. 
    Args:Â 
    - dt time step 
    - Î· learning step
    - update_ab :Â if false, the modes are freezed
    - update_jarczynski :Â if false, the weights are set to 1
    """

    if update_jarczynski
        W = exp.(S.w)
        Z = sum(W)
    else 
        W = ones(S.n)
        Z = S.n 
    end

    teacher =  sum(T.âˆ‡, dims=1) / T.n
    student =  sum( S.âˆ‡ .* W, dims=1) / Z
    Î´ = dt * Î· * ( student - teacher )

    if !update_ab
        Î´[2] = 0
        Î´[3] = 0
    end 

    S.s += Î´[1]
    S.a += Î´[2]
    S.b += Î´[3]
    S.p = Ïƒ(S.s)

    diffuse!(S, dt)
    âˆ‡S = âˆ‡U(S)
    âˆ‡T = âˆ‡U(T.x, S.s, S.a, S.b)

    if update_jarczynski
        S.w .-= 0.5 .* ( sum(S.âˆ‡ .* S.Î´, dims=2) .+ sum(âˆ‡S .* Î´, dims=2) )
    end

    S.Î´ = Î´
    T.âˆ‡ = âˆ‡T
    S.âˆ‡ = âˆ‡S    

end


function train_and_log( # main training function. 
    N_epochs; 
    K=100, 
    N=100, 
    Î·=0.01, 
    dt=0.01, 
    stride=100, # logging steps
    s=1.4, 
    a=-12, 
    b=9,
    s_init=0.00, 
    a_init = -12, 
    b_init = 9, 
    update_ab=false,
    update_jarczynski=false,
    initialize_at_data=false,
    log_fname="logs/test.bson")

    T = OracleSystem(n=N, s=s, a=a, b=b)
    update_ab ? 
        S = WeightedSystem(n=K, s=s_init, b=b_init, a=a_init) :Â 
        S = WeightedSystem(n=K, s=s_init, b=b, a=a) 
    
        args = Args(N_epochs=N_epochs, dt=dt, Î·=Î·, N=N, K=K, stride=stride)

    if initialize_at_data
        @assert K==N "Number of walkers and training data must match for data-init PCD"
        S.x = T.x
        S.âˆ‡ = T.âˆ‡
    end

    logger = create_logger(args, T)
    logger.qhat = sum( (T.x .<= 0) ) / T.n
    logger.ahat = sum( (T.x).*(T.x .<= 0)) / sum( (T.x .<= 0) )
    logger.bhat = sum( (T.x).*(T.x .>= 0)) / sum( (T.x .>= 0) )
    logger.pzero = sum( S.x .< 0) / S.n

    epochs = ProgressBar(1:args.N_epochs)

    for t in epochs
        update!(S,T,args.dt,args.Î·, update_ab, update_jarczynski)
        t%stride == 1 ? log!(1+tÃ·stride, logger, S) :Â nothing 
    end
    
    @save log_fname logger
    return S
end


function main(n = 1000)
    @info "Jarzynski correction"
    train_and_log(
        n; K=200, N=200, stride=10, update_jarczynski=true, initialize_at_data = false, log_fname="./JAR.bson", s_init=0.001)
    @info "PCD"
    train_and_log(
        n; K=200, N=200, stride=10, update_jarczynski=false, initialize_at_data = true, log_fname="./PCD.bson", s_init=0.001)
    @info "Unweighted evolution"
    train_and_log(
        n; K=200, N=200, stride=10, update_jarczynski=false, initialize_at_data = false, log_fname="./UNW.bson", s_init=0.001)
end 

# launch the program :Â > julia train.jl
if abspath(PROGRAM_FILE) == @__FILE__
    main(1000000)
end


